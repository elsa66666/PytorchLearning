{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/HR.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sales', 'accounting', 'hr', 'technical', 'support', 'management',\n",
       "       'IT', 'product_mng', 'marketing', 'RandD'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.part.unique() #part这一栏的所有出现过的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salary  part       \n",
       "high    IT               83\n",
       "        RandD            51\n",
       "        accounting       74\n",
       "        hr               45\n",
       "        management      225\n",
       "        marketing        80\n",
       "        product_mng      68\n",
       "        sales           269\n",
       "        support         141\n",
       "        technical       201\n",
       "low     IT              609\n",
       "        RandD           364\n",
       "        accounting      358\n",
       "        hr              335\n",
       "        management      180\n",
       "        marketing       402\n",
       "        product_mng     451\n",
       "        sales          2099\n",
       "        support        1146\n",
       "        technical      1372\n",
       "medium  IT              535\n",
       "        RandD           372\n",
       "        accounting      335\n",
       "        hr              359\n",
       "        management      225\n",
       "        marketing       376\n",
       "        product_mng     383\n",
       "        sales          1772\n",
       "        support         942\n",
       "        technical      1147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"salary\", \"part\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(pd.get_dummies(data.salary))   #salary这一列转化成one hot编码（将\"low\" \"high\"等属性转化成010这种数值）\n",
    "data = data.join(pd.get_dummies(data.part))\n",
    "del data[\"salary\"]\n",
    "del data[\"part\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>...</th>\n",
       "      <th>IT</th>\n",
       "      <th>RandD</th>\n",
       "      <th>accounting</th>\n",
       "      <th>hr</th>\n",
       "      <th>management</th>\n",
       "      <th>marketing</th>\n",
       "      <th>product_mng</th>\n",
       "      <th>sales</th>\n",
       "      <th>support</th>\n",
       "      <th>technical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.96</td>\n",
       "      <td>6</td>\n",
       "      <td>280</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       satisfaction_level  last_evaluation  number_project  \\\n",
       "0                    0.38             0.53               2   \n",
       "1                    0.80             0.86               5   \n",
       "2                    0.11             0.88               7   \n",
       "3                    0.72             0.87               5   \n",
       "4                    0.37             0.52               2   \n",
       "...                   ...              ...             ...   \n",
       "14994                0.40             0.57               2   \n",
       "14995                0.37             0.48               2   \n",
       "14996                0.37             0.53               2   \n",
       "14997                0.11             0.96               6   \n",
       "14998                0.37             0.52               2   \n",
       "\n",
       "       average_montly_hours  time_spend_company  Work_accident  left  \\\n",
       "0                       157                   3              0     1   \n",
       "1                       262                   6              0     1   \n",
       "2                       272                   4              0     1   \n",
       "3                       223                   5              0     1   \n",
       "4                       159                   3              0     1   \n",
       "...                     ...                 ...            ...   ...   \n",
       "14994                   151                   3              0     1   \n",
       "14995                   160                   3              0     1   \n",
       "14996                   143                   3              0     1   \n",
       "14997                   280                   4              0     1   \n",
       "14998                   158                   3              0     1   \n",
       "\n",
       "       promotion_last_5years  high  low  ...  IT  RandD  accounting  hr  \\\n",
       "0                          0     0    1  ...   0      0           0   0   \n",
       "1                          0     0    0  ...   0      0           0   0   \n",
       "2                          0     0    0  ...   0      0           0   0   \n",
       "3                          0     0    1  ...   0      0           0   0   \n",
       "4                          0     0    1  ...   0      0           0   0   \n",
       "...                      ...   ...  ...  ...  ..    ...         ...  ..   \n",
       "14994                      0     0    1  ...   0      0           0   0   \n",
       "14995                      0     0    1  ...   0      0           0   0   \n",
       "14996                      0     0    1  ...   0      0           0   0   \n",
       "14997                      0     0    1  ...   0      0           0   0   \n",
       "14998                      0     0    1  ...   0      0           0   0   \n",
       "\n",
       "       management  marketing  product_mng  sales  support  technical  \n",
       "0               0          0            0      1        0          0  \n",
       "1               0          0            0      1        0          0  \n",
       "2               0          0            0      1        0          0  \n",
       "3               0          0            0      1        0          0  \n",
       "4               0          0            0      1        0          0  \n",
       "...           ...        ...          ...    ...      ...        ...  \n",
       "14994           0          0            0      0        1          0  \n",
       "14995           0          0            0      0        1          0  \n",
       "14996           0          0            0      0        1          0  \n",
       "14997           0          0            0      0        1          0  \n",
       "14998           0          0            0      0        1          0  \n",
       "\n",
       "[14999 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11428\n",
       "1     3571\n",
       "Name: left, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.left.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "y_data = data.left.values.reshape(-1,1)\n",
    "Y = torch.from_numpy(y_data).type(torch.FloatTensor)\n",
    "x_data = data[[c for c in data.columns if c !=\"left\"]].values\n",
    "X = torch.from_numpy(x_data).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建模型\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义模型 \n",
    "\n",
    "nn.module 继承这个类 \n",
    "\n",
    "__init__ 初始化所有层 \n",
    "\n",
    "forward 定义模型的运算过程（前向传播的过程）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.liner_1 = nn.Linear(20, 64) #输入20个属性，输出64个属性\n",
    "        self.liner_2 = nn.Linear(64, 64)\n",
    "        self.liner_3 = nn.Linear(64, 1) #最终输出一个属性\n",
    "        #self.relu = nn.ReLU() #激活层\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.liner_1(input))\n",
    "        x = F.relu(self.liner_2(x))\n",
    "        x = torch.sigmoid(self.liner_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Model()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim =  get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.6722943782806396\n",
      "epoch: 1 loss: 0.6496067047119141\n",
      "epoch: 2 loss: 0.6405868530273438\n",
      "epoch: 3 loss: 0.6309906840324402\n",
      "epoch: 4 loss: 0.621131420135498\n",
      "epoch: 5 loss: 0.6248529553413391\n",
      "epoch: 6 loss: 0.62132728099823\n",
      "epoch: 7 loss: 0.6049251556396484\n",
      "epoch: 8 loss: 0.5975252985954285\n",
      "epoch: 9 loss: 0.5888492465019226\n",
      "epoch: 10 loss: 0.5762665271759033\n",
      "epoch: 11 loss: 0.5702469348907471\n",
      "epoch: 12 loss: 0.567098081111908\n",
      "epoch: 13 loss: 0.5644480586051941\n",
      "epoch: 14 loss: 0.5764145851135254\n",
      "epoch: 15 loss: 0.5662938952445984\n",
      "epoch: 16 loss: 0.5607945919036865\n",
      "epoch: 17 loss: 0.5647764801979065\n",
      "epoch: 18 loss: 0.5604814887046814\n",
      "epoch: 19 loss: 0.5593259334564209\n",
      "epoch: 20 loss: 0.5604063272476196\n",
      "epoch: 21 loss: 0.5610149502754211\n",
      "epoch: 22 loss: 0.5611802935600281\n",
      "epoch: 23 loss: 0.5615288615226746\n",
      "epoch: 24 loss: 0.5618180632591248\n",
      "epoch: 25 loss: 0.5621487498283386\n",
      "epoch: 26 loss: 0.5623683333396912\n",
      "epoch: 27 loss: 0.5626288056373596\n",
      "epoch: 28 loss: 0.5671791434288025\n",
      "epoch: 29 loss: 0.5592106580734253\n",
      "epoch: 30 loss: 0.5618374943733215\n",
      "epoch: 31 loss: 0.5636206865310669\n",
      "epoch: 32 loss: 0.5622993111610413\n",
      "epoch: 33 loss: 0.5637621283531189\n",
      "epoch: 34 loss: 0.5640584230422974\n",
      "epoch: 35 loss: 0.5642550587654114\n",
      "epoch: 36 loss: 0.5634772181510925\n",
      "epoch: 37 loss: 0.5635796785354614\n",
      "epoch: 38 loss: 0.5637636184692383\n",
      "epoch: 39 loss: 0.5636585354804993\n",
      "epoch: 40 loss: 0.5636077523231506\n",
      "epoch: 41 loss: 0.5633419156074524\n",
      "epoch: 42 loss: 0.5632470846176147\n",
      "epoch: 43 loss: 0.5631803870201111\n",
      "epoch: 44 loss: 0.5738664865493774\n",
      "epoch: 45 loss: 0.5724024176597595\n",
      "epoch: 46 loss: 0.5633149147033691\n",
      "epoch: 47 loss: 0.5607964396476746\n",
      "epoch: 48 loss: 0.5630128383636475\n",
      "epoch: 49 loss: 0.5601212978363037\n",
      "epoch: 50 loss: 0.5599455237388611\n",
      "epoch: 51 loss: 0.5598149299621582\n",
      "epoch: 52 loss: 0.5595637559890747\n",
      "epoch: 53 loss: 0.5593931674957275\n",
      "epoch: 54 loss: 0.559112548828125\n",
      "epoch: 55 loss: 0.5589028596878052\n",
      "epoch: 56 loss: 0.5586114525794983\n",
      "epoch: 57 loss: 0.5805569887161255\n",
      "epoch: 58 loss: 0.5624501705169678\n",
      "epoch: 59 loss: 0.5666086077690125\n",
      "epoch: 60 loss: 0.5611664652824402\n",
      "epoch: 61 loss: 0.5574440360069275\n",
      "epoch: 62 loss: 0.5573928952217102\n",
      "epoch: 63 loss: 0.5586980581283569\n",
      "epoch: 64 loss: 0.5568289160728455\n",
      "epoch: 65 loss: 0.5579010248184204\n",
      "epoch: 66 loss: 0.555560290813446\n",
      "epoch: 67 loss: 0.5560631155967712\n",
      "epoch: 68 loss: 0.5547834634780884\n",
      "epoch: 69 loss: 0.5566825270652771\n",
      "epoch: 70 loss: 0.5543497800827026\n",
      "epoch: 71 loss: 0.5691519379615784\n",
      "epoch: 72 loss: 0.5747708678245544\n",
      "epoch: 73 loss: 0.5530323386192322\n",
      "epoch: 74 loss: 0.5553267598152161\n",
      "epoch: 75 loss: 0.5533728003501892\n",
      "epoch: 76 loss: 0.5672230124473572\n",
      "epoch: 77 loss: 0.5519756078720093\n",
      "epoch: 78 loss: 0.5563660860061646\n",
      "epoch: 79 loss: 0.5432668328285217\n",
      "epoch: 80 loss: 0.5584503412246704\n",
      "epoch: 81 loss: 0.5421177744865417\n",
      "epoch: 82 loss: 0.5501556396484375\n",
      "epoch: 83 loss: 0.5432729125022888\n",
      "epoch: 84 loss: 0.5390901565551758\n",
      "epoch: 85 loss: 0.5353225469589233\n",
      "epoch: 86 loss: 0.5346153378486633\n",
      "epoch: 87 loss: 0.5379322171211243\n",
      "epoch: 88 loss: 0.53290855884552\n",
      "epoch: 89 loss: 0.5342710018157959\n",
      "epoch: 90 loss: 0.5354509353637695\n",
      "epoch: 91 loss: 0.5333269834518433\n",
      "epoch: 92 loss: 0.5373722314834595\n",
      "epoch: 93 loss: 0.527519166469574\n",
      "epoch: 94 loss: 0.5286111235618591\n",
      "epoch: 95 loss: 0.5259267687797546\n",
      "epoch: 96 loss: 0.5272142887115479\n",
      "epoch: 97 loss: 0.5250904560089111\n",
      "epoch: 98 loss: 0.5226278305053711\n",
      "epoch: 99 loss: 0.5322944521903992\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        start = i*batch\n",
    "        end = start + batch\n",
    "        x = X[start:end]\n",
    "        y = Y[start:end]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, 'loss:', loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5323, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(X),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset类改写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.6750981211662292\n",
      "epoch: 1 loss: 0.6760808229446411\n",
      "epoch: 2 loss: 0.6654475331306458\n",
      "epoch: 3 loss: 0.6556015014648438\n",
      "epoch: 4 loss: 0.6449224352836609\n",
      "epoch: 5 loss: 0.6266797780990601\n",
      "epoch: 6 loss: 0.6162722110748291\n",
      "epoch: 7 loss: 0.6237182021141052\n",
      "epoch: 8 loss: 0.5998333692550659\n",
      "epoch: 9 loss: 0.5928251147270203\n",
      "epoch: 10 loss: 0.5866075754165649\n",
      "epoch: 11 loss: 0.5811472535133362\n",
      "epoch: 12 loss: 0.5862952470779419\n",
      "epoch: 13 loss: 0.5814121961593628\n",
      "epoch: 14 loss: 0.5751276612281799\n",
      "epoch: 15 loss: 0.5712578892707825\n",
      "epoch: 16 loss: 0.5680497288703918\n",
      "epoch: 17 loss: 0.5657187700271606\n",
      "epoch: 18 loss: 0.5638326406478882\n",
      "epoch: 19 loss: 0.5624296069145203\n",
      "epoch: 20 loss: 0.5615627765655518\n",
      "epoch: 21 loss: 0.5608751773834229\n",
      "epoch: 22 loss: 0.560674250125885\n",
      "epoch: 23 loss: 0.5603113174438477\n",
      "epoch: 24 loss: 0.5605241060256958\n",
      "epoch: 25 loss: 0.560603678226471\n",
      "epoch: 26 loss: 0.560169517993927\n",
      "epoch: 27 loss: 0.5602093935012817\n",
      "epoch: 28 loss: 0.5605840682983398\n",
      "epoch: 29 loss: 0.5605791807174683\n",
      "epoch: 30 loss: 0.5625866055488586\n",
      "epoch: 31 loss: 0.5627256035804749\n",
      "epoch: 32 loss: 0.5611377358436584\n",
      "epoch: 33 loss: 0.5636003613471985\n",
      "epoch: 34 loss: 0.5630399584770203\n",
      "epoch: 35 loss: 0.5630509257316589\n",
      "epoch: 36 loss: 0.5631918907165527\n",
      "epoch: 37 loss: 0.5632736086845398\n",
      "epoch: 38 loss: 0.5630928874015808\n",
      "epoch: 39 loss: 0.5780373215675354\n",
      "epoch: 40 loss: 0.5685070157051086\n",
      "epoch: 41 loss: 0.5610793828964233\n",
      "epoch: 42 loss: 0.5611842274665833\n",
      "epoch: 43 loss: 0.5643668174743652\n",
      "epoch: 44 loss: 0.5684523582458496\n",
      "epoch: 45 loss: 0.5602271556854248\n",
      "epoch: 46 loss: 0.5603810548782349\n",
      "epoch: 47 loss: 0.5600302815437317\n",
      "epoch: 48 loss: 0.5601326823234558\n",
      "epoch: 49 loss: 0.5593893527984619\n",
      "epoch: 50 loss: 0.5595747828483582\n",
      "epoch: 51 loss: 0.5628567934036255\n",
      "epoch: 52 loss: 0.5675181746482849\n",
      "epoch: 53 loss: 0.5584444999694824\n",
      "epoch: 54 loss: 0.5573055744171143\n",
      "epoch: 55 loss: 0.5569624304771423\n",
      "epoch: 56 loss: 0.5571328401565552\n",
      "epoch: 57 loss: 0.5567091107368469\n",
      "epoch: 58 loss: 0.5561352372169495\n",
      "epoch: 59 loss: 0.5554185509681702\n",
      "epoch: 60 loss: 0.5541161894798279\n",
      "epoch: 61 loss: 0.5547359585762024\n",
      "epoch: 62 loss: 0.5538674592971802\n",
      "epoch: 63 loss: 0.5524101853370667\n",
      "epoch: 64 loss: 0.5621306896209717\n",
      "epoch: 65 loss: 0.5531739592552185\n",
      "epoch: 66 loss: 0.5508992075920105\n",
      "epoch: 67 loss: 0.5504037737846375\n",
      "epoch: 68 loss: 0.5502197742462158\n",
      "epoch: 69 loss: 0.5453999042510986\n",
      "epoch: 70 loss: 0.5488093495368958\n",
      "epoch: 71 loss: 0.5431452989578247\n",
      "epoch: 72 loss: 0.5458477139472961\n",
      "epoch: 73 loss: 0.5429791212081909\n",
      "epoch: 74 loss: 0.5455165505409241\n",
      "epoch: 75 loss: 0.5412462949752808\n",
      "epoch: 76 loss: 0.5436147451400757\n",
      "epoch: 77 loss: 0.539509117603302\n",
      "epoch: 78 loss: 0.5401008725166321\n",
      "epoch: 79 loss: 0.5381066799163818\n",
      "epoch: 80 loss: 0.5373252034187317\n",
      "epoch: 81 loss: 0.5348654985427856\n",
      "epoch: 82 loss: 0.5345748662948608\n",
      "epoch: 83 loss: 0.5326442122459412\n",
      "epoch: 84 loss: 0.5317896604537964\n",
      "epoch: 85 loss: 0.5306872725486755\n",
      "epoch: 86 loss: 0.5295807719230652\n",
      "epoch: 87 loss: 0.5285751223564148\n",
      "epoch: 88 loss: 0.5274366736412048\n",
      "epoch: 89 loss: 0.5263389945030212\n",
      "epoch: 90 loss: 0.5251002311706543\n",
      "epoch: 91 loss: 0.5239585041999817\n",
      "epoch: 92 loss: 0.521820604801178\n",
      "epoch: 93 loss: 0.5211368799209595\n",
      "epoch: 94 loss: 0.5189903974533081\n",
      "epoch: 95 loss: 0.5177356004714966\n",
      "epoch: 96 loss: 0.5163467526435852\n",
      "epoch: 97 loss: 0.5193750858306885\n",
      "epoch: 98 loss: 0.5148670673370361\n",
      "epoch: 99 loss: 0.5130397081375122\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "model, optim = get_model()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches):\n",
    "        x, y = HRdataset[i*batch: i*batch+batch]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, 'loss:', loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.5595817565917969\n",
      "epoch: 1 loss: 0.5561066269874573\n",
      "epoch: 2 loss: 0.5518569350242615\n",
      "epoch: 3 loss: 0.5420863628387451\n",
      "epoch: 4 loss: 0.5354121327400208\n",
      "epoch: 5 loss: 0.5283897519111633\n",
      "epoch: 6 loss: 0.5198034048080444\n",
      "epoch: 7 loss: 0.5063390135765076\n",
      "epoch: 8 loss: 0.4922177493572235\n",
      "epoch: 9 loss: 0.48073360323905945\n",
      "epoch: 10 loss: 0.4670585095882416\n",
      "epoch: 11 loss: 0.45394015312194824\n",
      "epoch: 12 loss: 0.44192153215408325\n",
      "epoch: 13 loss: 0.43051043152809143\n",
      "epoch: 14 loss: 0.41850537061691284\n",
      "epoch: 15 loss: 0.40823349356651306\n",
      "epoch: 16 loss: 0.40107664465904236\n",
      "epoch: 17 loss: 0.40724125504493713\n",
      "epoch: 18 loss: 0.3798242211341858\n",
      "epoch: 19 loss: 0.3748375177383423\n",
      "epoch: 20 loss: 0.3651047646999359\n",
      "epoch: 21 loss: 0.3628564178943634\n",
      "epoch: 22 loss: 0.35356998443603516\n",
      "epoch: 23 loss: 0.34804242849349976\n",
      "epoch: 24 loss: 0.3507224917411804\n",
      "epoch: 25 loss: 0.33642080426216125\n",
      "epoch: 26 loss: 0.3355002701282501\n",
      "epoch: 27 loss: 0.32720696926116943\n",
      "epoch: 28 loss: 0.32677215337753296\n",
      "epoch: 29 loss: 0.3255283236503601\n",
      "epoch: 30 loss: 0.3156561851501465\n",
      "epoch: 31 loss: 0.31231555342674255\n",
      "epoch: 32 loss: 0.30976295471191406\n",
      "epoch: 33 loss: 0.3129994869232178\n",
      "epoch: 34 loss: 0.3041094243526459\n",
      "epoch: 35 loss: 0.30325549840927124\n",
      "epoch: 36 loss: 0.2994687855243683\n",
      "epoch: 37 loss: 0.3126925528049469\n",
      "epoch: 38 loss: 0.29570668935775757\n",
      "epoch: 39 loss: 0.2932109534740448\n",
      "epoch: 40 loss: 0.29018235206604004\n",
      "epoch: 41 loss: 0.29129377007484436\n",
      "epoch: 42 loss: 0.28752613067626953\n",
      "epoch: 43 loss: 0.28462836146354675\n",
      "epoch: 44 loss: 0.28562837839126587\n",
      "epoch: 45 loss: 0.2915038764476776\n",
      "epoch: 46 loss: 0.2804188132286072\n",
      "epoch: 47 loss: 0.2816621661186218\n",
      "epoch: 48 loss: 0.28135600686073303\n",
      "epoch: 49 loss: 0.28105857968330383\n",
      "epoch: 50 loss: 0.27743691205978394\n",
      "epoch: 51 loss: 0.27415239810943604\n",
      "epoch: 52 loss: 0.27565377950668335\n",
      "epoch: 53 loss: 0.27745333313941956\n",
      "epoch: 54 loss: 0.2703298330307007\n",
      "epoch: 55 loss: 0.27308496832847595\n",
      "epoch: 56 loss: 0.27847611904144287\n",
      "epoch: 57 loss: 0.2802852690219879\n",
      "epoch: 58 loss: 0.2681047320365906\n",
      "epoch: 59 loss: 0.27177080512046814\n",
      "epoch: 60 loss: 0.2652333974838257\n",
      "epoch: 61 loss: 0.2675703465938568\n",
      "epoch: 62 loss: 0.2702033221721649\n",
      "epoch: 63 loss: 0.26529964804649353\n",
      "epoch: 64 loss: 0.2621678411960602\n",
      "epoch: 65 loss: 0.2643955647945404\n",
      "epoch: 66 loss: 0.2614268958568573\n",
      "epoch: 67 loss: 0.2598161995410919\n",
      "epoch: 68 loss: 0.25978410243988037\n",
      "epoch: 69 loss: 0.27016550302505493\n",
      "epoch: 70 loss: 0.26430410146713257\n",
      "epoch: 71 loss: 0.2569519877433777\n",
      "epoch: 72 loss: 0.25600242614746094\n",
      "epoch: 73 loss: 0.2617694139480591\n",
      "epoch: 74 loss: 0.2603466808795929\n",
      "epoch: 75 loss: 0.2558061480522156\n",
      "epoch: 76 loss: 0.25446343421936035\n",
      "epoch: 77 loss: 0.2742801904678345\n",
      "epoch: 78 loss: 0.26328930258750916\n",
      "epoch: 79 loss: 0.2507683038711548\n",
      "epoch: 80 loss: 0.2545297145843506\n",
      "epoch: 81 loss: 0.24948111176490784\n",
      "epoch: 82 loss: 0.24887682497501373\n",
      "epoch: 83 loss: 0.2546581029891968\n",
      "epoch: 84 loss: 0.24900656938552856\n",
      "epoch: 85 loss: 0.2464260458946228\n",
      "epoch: 86 loss: 0.2527048885822296\n",
      "epoch: 87 loss: 0.2462649643421173\n",
      "epoch: 88 loss: 0.2448587566614151\n",
      "epoch: 89 loss: 0.2539120018482208\n",
      "epoch: 90 loss: 0.2467578947544098\n",
      "epoch: 91 loss: 0.2460007518529892\n",
      "epoch: 92 loss: 0.24666070938110352\n",
      "epoch: 93 loss: 0.24611680209636688\n",
      "epoch: 94 loss: 0.24419140815734863\n",
      "epoch: 95 loss: 0.25081196427345276\n",
      "epoch: 96 loss: 0.24228067696094513\n",
      "epoch: 97 loss: 0.24079455435276031\n",
      "epoch: 98 loss: 0.24028444290161133\n",
      "epoch: 99 loss: 0.24461005628108978\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "HR_ds = TensorDataset(X,Y)\n",
    "HR_dl = DataLoader(HR_ds, batch_size=batch, shuffle=True) #HR dataloader, shuffle:是否乱序\n",
    "model, optim = get_model()\n",
    "from torch.utils.data import TensorDataset\n",
    "HRdataset = TensorDataset(X, Y)\n",
    "model, optim = get_model()\n",
    "for epoch in range(epochs):\n",
    "    for x,y in HR_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        print('epoch:', epoch, 'loss:', loss_fn(model(X),Y).data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过拟合：对于训练数据过度拟合，对未知数据预测很差\n",
    "#### 欠拟合：对于训练数据拟合不够，对未知数据预测很差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_data, y_data)\n",
    "\n",
    "train_x = torch.from_numpy(train_x).type(torch.float32)\n",
    "train_y = torch.from_numpy(train_y).type(torch.float32)\n",
    "test_x = torch.from_numpy(test_x).type(torch.float32)\n",
    "test_y = torch.from_numpy(test_y).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True) #HR dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_x, test_y)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch) #HR dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算正确率 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = (y_pred > 0.5).type(torch.int32)  #signoid函数值大于0.5预测为1\n",
    "\n",
    "(y_pred == labels).float().mean() #计算预测正确的概率，作为正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    y_pred = (y_pred > 0.5).type(torch.int32)\n",
    "    acc = (y_pred == y_true).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.5608251690864563 accuracy: 0.7619174718856812\n",
      "epoch: 1 loss: 0.5604106783866882 accuracy: 0.7619174718856812\n",
      "epoch: 2 loss: 0.5589512586593628 accuracy: 0.7619174718856812\n",
      "epoch: 3 loss: 0.5549231767654419 accuracy: 0.7619174718856812\n",
      "epoch: 4 loss: 0.5524945855140686 accuracy: 0.7619174718856812\n",
      "epoch: 5 loss: 0.5485906004905701 accuracy: 0.7619174718856812\n",
      "epoch: 6 loss: 0.5447193384170532 accuracy: 0.7619174718856812\n",
      "epoch: 7 loss: 0.5397374033927917 accuracy: 0.7619174718856812\n",
      "epoch: 8 loss: 0.5353987216949463 accuracy: 0.7619174718856812\n",
      "epoch: 9 loss: 0.5331559777259827 accuracy: 0.7619174718856812\n",
      "epoch: 10 loss: 0.5289856791496277 accuracy: 0.7619174718856812\n",
      "epoch: 11 loss: 0.524376392364502 accuracy: 0.7619174718856812\n",
      "epoch: 12 loss: 0.5220655202865601 accuracy: 0.7619174718856812\n",
      "epoch: 13 loss: 0.5115835666656494 accuracy: 0.7619174718856812\n",
      "epoch: 14 loss: 0.5050650238990784 accuracy: 0.7619174718856812\n",
      "epoch: 15 loss: 0.5013545751571655 accuracy: 0.7619174718856812\n",
      "epoch: 16 loss: 0.495146781206131 accuracy: 0.7619174718856812\n",
      "epoch: 17 loss: 0.49282118678092957 accuracy: 0.7619174718856812\n",
      "epoch: 18 loss: 0.48481881618499756 accuracy: 0.7619174718856812\n",
      "epoch: 19 loss: 0.47732043266296387 accuracy: 0.7619174718856812\n",
      "epoch: 20 loss: 0.47210827469825745 accuracy: 0.7585172057151794\n",
      "epoch: 21 loss: 0.4649074971675873 accuracy: 0.7599173188209534\n",
      "epoch: 22 loss: 0.45999300479888916 accuracy: 0.7573838233947754\n",
      "epoch: 23 loss: 0.4542844295501709 accuracy: 0.7593172788619995\n",
      "epoch: 24 loss: 0.4617452323436737 accuracy: 0.7611173987388611\n",
      "epoch: 25 loss: 0.4444730877876282 accuracy: 0.7568504810333252\n",
      "epoch: 26 loss: 0.43903982639312744 accuracy: 0.7590506076812744\n",
      "epoch: 27 loss: 0.4333730638027191 accuracy: 0.7587172389030457\n",
      "epoch: 28 loss: 0.4284210503101349 accuracy: 0.7584505677223206\n",
      "epoch: 29 loss: 0.4265727400779724 accuracy: 0.7555837035179138\n",
      "epoch: 30 loss: 0.4202403128147125 accuracy: 0.7855857014656067\n",
      "epoch: 31 loss: 0.4140477180480957 accuracy: 0.7729182243347168\n",
      "epoch: 32 loss: 0.4106868505477905 accuracy: 0.8096539974212646\n",
      "epoch: 33 loss: 0.4058385491371155 accuracy: 0.802853524684906\n",
      "epoch: 34 loss: 0.4129478335380554 accuracy: 0.8419894576072693\n",
      "epoch: 35 loss: 0.40181028842926025 accuracy: 0.8333222270011902\n",
      "epoch: 36 loss: 0.3994693160057068 accuracy: 0.8414561152458191\n",
      "epoch: 37 loss: 0.4007164537906647 accuracy: 0.7589172720909119\n",
      "epoch: 38 loss: 0.39966219663619995 accuracy: 0.759984016418457\n",
      "epoch: 39 loss: 0.38425952196121216 accuracy: 0.8386558890342712\n",
      "epoch: 40 loss: 0.3854611814022064 accuracy: 0.7819854617118835\n",
      "epoch: 41 loss: 0.37718477845191956 accuracy: 0.8389226198196411\n",
      "epoch: 42 loss: 0.3744395673274994 accuracy: 0.8426561951637268\n",
      "epoch: 43 loss: 0.38487622141838074 accuracy: 0.8543236255645752\n",
      "epoch: 44 loss: 0.3731725215911865 accuracy: 0.7989199161529541\n",
      "epoch: 45 loss: 0.36552658677101135 accuracy: 0.8495233058929443\n",
      "epoch: 46 loss: 0.3668198585510254 accuracy: 0.79778653383255\n",
      "epoch: 47 loss: 0.36268752813339233 accuracy: 0.8553903698921204\n",
      "epoch: 48 loss: 0.35748589038848877 accuracy: 0.839522659778595\n",
      "epoch: 49 loss: 0.3553517758846283 accuracy: 0.8415894508361816\n",
      "epoch: 50 loss: 0.35385727882385254 accuracy: 0.8335888981819153\n",
      "epoch: 51 loss: 0.35127267241477966 accuracy: 0.8367891311645508\n",
      "epoch: 52 loss: 0.3471531867980957 accuracy: 0.8500566482543945\n",
      "epoch: 53 loss: 0.37197646498680115 accuracy: 0.7850523591041565\n",
      "epoch: 54 loss: 0.34441643953323364 accuracy: 0.8602573275566101\n",
      "epoch: 55 loss: 0.3439636528491974 accuracy: 0.8365890979766846\n",
      "epoch: 56 loss: 0.3380623161792755 accuracy: 0.8615241050720215\n",
      "epoch: 57 loss: 0.33657675981521606 accuracy: 0.8637909293174744\n",
      "epoch: 58 loss: 0.3356248736381531 accuracy: 0.8696579933166504\n",
      "epoch: 59 loss: 0.3355581760406494 accuracy: 0.8753917217254639\n",
      "epoch: 60 loss: 0.3327801823616028 accuracy: 0.8723915219306946\n",
      "epoch: 61 loss: 0.3495623469352722 accuracy: 0.8049870133399963\n",
      "epoch: 62 loss: 0.32648250460624695 accuracy: 0.8673244714736938\n",
      "epoch: 63 loss: 0.3288007080554962 accuracy: 0.8766584396362305\n",
      "epoch: 64 loss: 0.3246476948261261 accuracy: 0.8755916953086853\n",
      "epoch: 65 loss: 0.32315096259117126 accuracy: 0.8646576404571533\n",
      "epoch: 66 loss: 0.3204951584339142 accuracy: 0.8659244179725647\n",
      "epoch: 67 loss: 0.320087194442749 accuracy: 0.8791252970695496\n",
      "epoch: 68 loss: 0.317489892244339 accuracy: 0.8657910823822021\n",
      "epoch: 69 loss: 0.3146826922893524 accuracy: 0.8781918883323669\n",
      "epoch: 70 loss: 0.31306910514831543 accuracy: 0.8803920149803162\n",
      "epoch: 71 loss: 0.3209773898124695 accuracy: 0.8859257102012634\n",
      "epoch: 72 loss: 0.3162968158721924 accuracy: 0.8573238253593445\n",
      "epoch: 73 loss: 0.313570111989975 accuracy: 0.8654577136039734\n",
      "epoch: 74 loss: 0.30773353576660156 accuracy: 0.8866590857505798\n",
      "epoch: 75 loss: 0.3085123598575592 accuracy: 0.8720581531524658\n",
      "epoch: 76 loss: 0.3058287799358368 accuracy: 0.8759250640869141\n",
      "epoch: 77 loss: 0.30533307790756226 accuracy: 0.8898593187332153\n",
      "epoch: 78 loss: 0.3102434277534485 accuracy: 0.8897926807403564\n",
      "epoch: 79 loss: 0.30116716027259827 accuracy: 0.8813254237174988\n",
      "epoch: 80 loss: 0.2998133897781372 accuracy: 0.8918594717979431\n",
      "epoch: 81 loss: 0.2989806532859802 accuracy: 0.8817254304885864\n",
      "epoch: 82 loss: 0.29745182394981384 accuracy: 0.8932595252990723\n",
      "epoch: 83 loss: 0.299907386302948 accuracy: 0.8769918084144592\n",
      "epoch: 84 loss: 0.2994132936000824 accuracy: 0.8931928873062134\n",
      "epoch: 85 loss: 0.2936176359653473 accuracy: 0.8894593119621277\n",
      "epoch: 86 loss: 0.29194843769073486 accuracy: 0.8905926942825317\n",
      "epoch: 87 loss: 0.29429900646209717 accuracy: 0.8938595652580261\n",
      "epoch: 88 loss: 0.29926666617393494 accuracy: 0.8911260962486267\n",
      "epoch: 89 loss: 0.289293110370636 accuracy: 0.8956596851348877\n",
      "epoch: 90 loss: 0.29225432872772217 accuracy: 0.8941929340362549\n",
      "epoch: 91 loss: 0.29395562410354614 accuracy: 0.894126296043396\n",
      "epoch: 92 loss: 0.2928643524646759 accuracy: 0.8965930938720703\n",
      "epoch: 93 loss: 0.2910527288913727 accuracy: 0.8807253837585449\n",
      "epoch: 94 loss: 0.28333887457847595 accuracy: 0.8911927342414856\n",
      "epoch: 95 loss: 0.28600725531578064 accuracy: 0.8951263427734375\n",
      "epoch: 96 loss: 0.2793579399585724 accuracy: 0.8971931338310242\n",
      "epoch: 97 loss: 0.2850300669670105 accuracy: 0.8844589591026306\n",
      "epoch: 98 loss: 0.2773953676223755 accuracy: 0.8983932137489319\n",
      "epoch: 99 loss: 0.27542009949684143 accuracy: 0.8995932936668396\n"
     ]
    }
   ],
   "source": [
    "model, optim = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x,y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    with torch.no_grad():\n",
    "        epoch_accuracy = accuracy(model(X), Y) #accuracy(预测的y, 真实的y)\n",
    "        print('epoch:', epoch, 'loss:', loss_fn(model(X),Y).data.item(), 'accuracy:', epoch_accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
